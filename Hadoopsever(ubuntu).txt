#安装中文语言包
sudo apt update
sudo apt install language-pack-zh-hans language-pack-zh-hans-base
sudo update-locale LANG=zh_CN.UTF-8

#编辑网络配置
sudo nano /etc/netplan/00-installer-config.yaml
内容：
network:
  version: 2
  renderer: networkd
  ethernets:
    ens33:
      dhcp4: no
      dhcp6: no
      addresses: [192.168.115.128/24]
      routes:
        - to: default
          via: 192.168.115.2
      nameservers:
        addresses: [223.5.5.5, 114.114.114.114, 8.8.8.8]

sudo chmod 600 /etc/netplan/00-installer-config.yaml
sudo netplan apply
#重启网络服务
sudo systemctl restart systemd-networkd

# 修改主机名
sudo hostnamectl set-hostname master
sudo hostnamectl set-hostname node1
sudo hostnamectl set-hostname node2


#更换镜像（所有节点）
sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak
阿里云：
sudo tee /etc/apt/sources.list <<'EOF'
deb https://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiverse
deb https://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiverse
deb https://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiverse
deb https://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiverse
EOF
清华源：
sudo tee /etc/apt/sources.list <<'EOF'
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse
EOF

sudo apt update #更新索引


#下载Hadoop（master）
cd ~
# 下载稳定版本 (例如 3.3.6)
wget -c https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz

# 安装Java11（所有节点）
sudo apt update
sudo apt install -y openjdk-11-jdk
java -version
javac -version
ls /usr/lib/jvm/

# 解压（master）
tar -xzf hadoop-3.3.6.tar.gz
sudo mv hadoop-3.3.6 /usr/local/hadoop # 移动到 /usr/local 并重命名
sudo chown -R cy:cy /usr/local/hadoop # 将所有权给当前用户 cy

#配置环境变量（所有节点）
nano ~/.bashrc

# Hadoop Environment Variables
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64 # 使用你的Java路径

rocky linux
# Hadoop 路径优化
export HADOOP_HOME=/opt/module/hadoop 
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk

source ~/.bashrc

sudo nano /etc/hosts
sudo vim /etc/hosts
192.168.32.128 master
192.168.32.129 node1
192.168.32.130 node2

cat <<EOF | sudo tee -a /etc/hosts
192.168.32.131 master
192.168.32.132 node1
192.168.32.133 node2
EOF

192.168.32.131 master
192.168.32.132 node1
192.168.32.133 node2



#把 cy 加入 sudoers 并配置 scp 免密以便分发文件及集群运行
sudo usermod -aG sudo cy #每个节点
配置免密sudo
sudo visudo
cy ALL=(ALL) NOPASSWD: ALL #放在文件最末尾

生成密钥并分发（master）
ssh-keygen -t rsa
ssh-copy-id cy@master
ssh-copy-id cy@node1
ssh-copy-id cy@node2

for host in master node1 node2; do
    ssh $host "hostname; java -version"
done

如果前面分发未成功，手动分发（node1，node2）
cat ~/.ssh/id_ed25519.pub
mkdir -p ~/.ssh
chmod 700 ~/.ssh
echo 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIet8pIk2dyB3NZIQQjhYCXZ+QzCFWlEUJJHixkkF+mQ cy@master' >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys


#配置Hadoop核心文件（仅在master节点）
/usr/local/hadoop/etc/hadoop/
core-site.xml：
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/cy/hadoop_data/tmp</value>
    </property>
</configuration>

hdfs-site.xml：
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file://${hadoop.tmp.dir}/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file://${hadoop.tmp.dir}/dfs/data</value>
    </property>
</configuration>

mapred-site.xml：
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>

yarn-site.xml：
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ</value>
    </property>
</configuration>

workers：
node1
node2

hadoop-env.sh：
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

#分发Hadoop到node1，node2
cd /usr/local
rsync -avz --rsync-path='sudo rsync' /usr/local/hadoop/ cy@node1:/usr/local/hadoop/
rsync -avz --rsync-path='sudo rsync' /usr/local/hadoop/ cy@node2:/usr/local/hadoop/

#启动/关闭Hadoop集群
hdfs namenode -format #格式化（首次启动）
先启动HDFS，再启动Yarn
start-dfs.sh
start-yarn.sh
先停Yarn，再停HDFS
stop-yarn.sh
stop-dfs.sh

sudo mkdir -p /mnt/windows
sudo vmhgfs-fuse .host:/Ubuntu1 /mnt/windows -o allow_other -o uid=1000
sudo vmhgfs-fuse .host:/Ubuntu1 /mnt/windows -o allow_other -o nonempty -o uid=1000
ls /mnt/windows/

#验证集群状态
jps

master：
NameNode
SecondaryNameNode
ResourceManager

node：
DataNode
NodeManager

#宿主浏览器查看web面板
http://192.168.115.128:9870
http://192.168.115.128:8088

hdfs dfs -mkdir -p /user/input
hdfs dfs -put /mnt/windows/input.txt /user/input/
hdfs dfs -ls /user/input/
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /user/input /user/output
hdfs dfs -ls /user/output
hdfs dfs -cat /user/output/part-r-00000